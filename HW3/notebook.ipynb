{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 5. \n",
    "Train a Linear Regression model, an MLP Regressor with 2 hidden layers of 10 \\\n",
    "neurons each and no activation functions, and another MLP Regressor with 2 hidden \\\n",
    "layers of 10 neurons each using ReLU activation functions. (Use `random_state=0` on the \\\n",
    "MLPs, regardless of the run). Plot a boxplot of the test MAE of each model.\n",
    "\n",
    "> average the performance of the models over 10 separate runs. In each \\\n",
    "> run, use a different 80-20 train-test split by setting a random_state=i, with i=1..10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T10:22:00.963590Z",
     "iopub.status.busy": "2024-10-17T10:22:00.963234Z",
     "iopub.status.idle": "2024-10-17T10:23:17.341671Z",
     "shell.execute_reply": "2024-10-17T10:23:17.340454Z",
     "shell.execute_reply.started": "2024-10-17T10:22:00.963552Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code for ex5\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress convergence warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "df = pd.read_csv('parkinsons.csv')\n",
    "X = df.drop(columns=['target'])  \n",
    "y = df['target']\n",
    "\n",
    "# Inicializar listas para armazenar os MAE de cada modelo\n",
    "mae_linear = []\n",
    "mae_mlp_no_activation = []\n",
    "mae_mlp_relu = []\n",
    "\n",
    "# Executar o ciclo de 10 iterações com divisões diferentes (random_state=i)\n",
    "for i in range(1, 11):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "\n",
    "    # Regressão Linear\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(X_train, y_train)\n",
    "    y_pred_linear = linear_model.predict(X_test)\n",
    "    mae_linear.append(mean_absolute_error(y_test, y_pred_linear))\n",
    "\n",
    "    # MLP sem funções de ativação\n",
    "    mlp_no_activation = MLPRegressor(hidden_layer_sizes=(10, 10), activation='identity', random_state=0)\n",
    "    mlp_no_activation.fit(X_train, y_train)\n",
    "    y_pred_no_activation = mlp_no_activation.predict(X_test)\n",
    "    mae_mlp_no_activation.append(mean_absolute_error(y_test, y_pred_no_activation))\n",
    "\n",
    "    # MLP com ReLU\n",
    "    mlp_relu = MLPRegressor(hidden_layer_sizes=(10, 10), activation='relu', random_state=0)\n",
    "    mlp_relu.fit(X_train, y_train)\n",
    "    y_pred_relu = mlp_relu.predict(X_test)\n",
    "    mae_mlp_relu.append(mean_absolute_error(y_test, y_pred_relu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplots\n",
    "labels = ['Linear Regression', 'MLP No Activation', 'MLP ReLU']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=[mae_linear, mae_mlp_no_activation, mae_mlp_relu])\n",
    "plt.xticks(ticks=np.arange(len(labels)), labels=labels)\n",
    "plt.ylabel('Test MAE')\n",
    "plt.title('Comparison of Test MAE Across Models')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 6.\n",
    "Compare a Linear Regression with a MLP with no activations, and explain the impact \\\n",
    "and the importance of using activation functions in a MLP. Support your reasoning with the \\\n",
    "results from the boxplots.\n",
    "\n",
    "> Refer to the report for the explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 7.\n",
    "Using a 80-20 train-test split with `random_state=0`, use a Grid Search to tune the \\\n",
    "hyperparameters of an MLP regressor with two hidden layers (size 10 each). The \\\n",
    "parameters to search over are: (i) L2 penalty, with the values $\\{0.0001, 0.001, 0.01\\}$; (ii) \\\n",
    "learning rate, with the values $\\{0.001, 0.01, 0.1\\}$; and (iii) batch size, with the values \\\n",
    "$\\{32, 64, 128\\}$. Plot the test MAE for each combination of hyperparameters, report the \\\n",
    "best combination, and discuss the trade-offs between the combinations.\n",
    "\n",
    "> Refer to the report for the explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T10:23:19.990295Z",
     "iopub.status.busy": "2024-10-17T10:23:19.990000Z",
     "iopub.status.idle": "2024-10-17T10:29:03.806535Z",
     "shell.execute_reply": "2024-10-17T10:29:03.803492Z",
     "shell.execute_reply.started": "2024-10-17T10:23:19.990257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code for ex7\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Carregar o dataset\n",
    "df = pd.read_csv('parkinsons.csv')\n",
    "\n",
    "# Separar as variáveis preditoras (X) e a variável alvo (y)\n",
    "X = df.drop(columns=['target'])  # Supondo que 'target' é a variável de saída\n",
    "y = df['target']\n",
    "\n",
    "# Dividir o conjunto de dados em treino e teste (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Definir os hiperparâmetros para o GridSearch\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01],  # Penalidade L2\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1],  # Taxa de aprendizagem inicial\n",
    "    'batch_size': [32, 64, 128]  # Tamanho do batch\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Grid Search\n",
    "for alpha in param_grid['alpha']:\n",
    "    for learning_rate_init in param_grid['learning_rate_init']:\n",
    "        for batch_size in param_grid['batch_size']:\n",
    "            # Initialize MLP Regressor\n",
    "            model = MLPRegressor(hidden_layer_sizes=(10, 10), \n",
    "                                 alpha=alpha, \n",
    "                                 learning_rate_init=learning_rate_init, \n",
    "                                 batch_size=batch_size, \n",
    "                                 random_state=0)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "            results.append({\n",
    "                'alpha': alpha,\n",
    "                'learning_rate_init': learning_rate_init,\n",
    "                'batch_size': batch_size,\n",
    "                'mae': mae\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and show results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "vmin = min(results_df['mae'])\n",
    "vmax = max(results_df['mae'])\n",
    "\n",
    "for i, batch_size in enumerate(param_grid['batch_size']):\n",
    "    subset = results_df[results_df['batch_size'] == batch_size]\n",
    "    \n",
    "    # Create pivot for each heatmap\n",
    "    heatmap_data = subset.pivot(index='alpha', columns='learning_rate_init', values='mae')\n",
    "    \n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='RdYlGn_r', fmt=\".5f\", linewidths=0.5, ax=axes[i], vmin=vmin, vmax=vmax, cbar=False)\n",
    "    \n",
    "    axes[i].set_title(f'Batch Size = {batch_size}')\n",
    "    axes[i].set_xlabel('Learning Rate Init')\n",
    "    axes[i].set_ylabel('Alpha')\n",
    "\n",
    "cbar = fig.colorbar(axes[0].collections[0], ax=axes, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Test MAE Score')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best Hyperparameters: {results_df.loc[results_df['mae'].idxmin()].drop('mae').to_dict()}\")\n",
    "print(f\"Best Test MAE: {vmin}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
